{
  "hash": "1cb0b614bd34b7f1ea216b22794c1a1f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Non-linear modelling\"\ndescription: \"Beyond simple linear regression\"\ndate: \"2025-02-13\"\nformat:\n  html:\n    page-layout: full\n    html-math-method: katex\ncitation:\n  url: \"https://c-monaghan.github.io/posts/2025/\"\nexecute: \n  warning: false\neditor: source\n---\n\n::: {.cell}\n\n:::\n\n\n\nFor regression, the standard linear model:\n\n$$\ny_i \\sim N(\\mu_i, \\; \\sigma^2) \\qquad \\mu_i = \\alpha + \\sum^K_{k=1}\\beta_kx_{ik} + \\epsilon\n$$\n\nis a widely used approach to describe the relationship between a response variable $y_i$ and a set of predictors $x_{ik}$. Linear models are popular due to their simplicity, ease of implementation, and interpretability. The coefficients provide direct insights into the relationship between each predictor and the response, making it straightforward to infer the impact of individual variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generating some data\nx <- seq(-10, 10, length.out = 100)\ny_linear <- x + rnorm(length(x), mean = 0, sd = 2)\ny_quadratic <- x^2 + rnorm(length(x), mean = 0, sd = 4)\ny_cubic <- x^3 + rnorm(length(x), mean = 0, sd = 8)\ny_sinusoidal <- sin(x) + rnorm(length(x), mean = 0, sd = 0.3)\n\ndata <- data.frame(\n  x = x, \n  y_linear = y_linear,\n  y_quadratic = y_quadratic, \n  y_cubic = y_cubic,\n  y_sinusoidal = y_sinusoidal)\n\n# Plotting\np_1 <- data |>\n  ggplot(aes(x = x, y = y_linear)) +\n  geom_point(size = 1.5, colour = \"steelblue4\") +\n  labs(title = \"Linear relationship\", y = \"y\")\n\np_2 <- data |>\n  ggplot(aes(x = x, y = y_quadratic)) +\n  geom_point(colour = \"steelblue4\") +\n  labs(title = \"Quadratic relationship\", y = \"y\")\n\np_3 <- data |>\n  ggplot(aes(x = x, y = y_cubic)) +\n  geom_point(colour = \"steelblue4\") +\n  labs(title = \"Cubic relationship\", y = \"y\")\n\np_4 <- data |>\n  ggplot(aes(x = x, y = y_sinusoidal)) +\n  geom_point(colour = \"steelblue4\") +\n  labs(title = \"Sinusoidal relationship\", y = \"y\")\n\n# Arrange plots in a grid layout\n(p_1 + p_2) / (p_3 + p_4)\n```\n\n::: {.cell-output-display}\n![Comparison of linear and nonlinear relationships. Each panel illustrates a different relationship between x and y (linear, quadratic, cubic, and sinusoidal)](index_files/figure-html/fig-non-linear-1.png){#fig-non-linear width=768}\n:::\n:::\n\n\n\n\nHowever, the standard linear model relies on the assumption of linearity, which is often an approximation of the true underlying relationship. In many real-world scenarios, this assumption may not hold (@fig-non-linear), leading to significant limitations in predictive accuracy. When the true relationship is nonlinear, a linear model may fail to capture complex patterns, resulting in poor performance. \n\nWe can see this in the figure below where our straight-line equation captures the data well in the top left plot, but fails to do for the remaining plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_1 <- p_1 + geom_smooth(\n  method = \"lm\", colour = \"firebrick4\", linewidth = 1.25, se = FALSE)\n\np_2 <- p_2 + geom_smooth(\n  method = \"lm\", colour = \"firebrick4\", linewidth = 1.25, se = FALSE)\n\np_3 <- p_3 + geom_smooth(\n  method = \"lm\", colour = \"firebrick4\", linewidth = 1.25, se = FALSE)\n\np_4 <- p_4 + geom_smooth(\n  method = \"lm\", colour = \"firebrick4\", linewidth = 1.25, se = FALSE)\n\n(p_1 + p_2) / (p_3 + p_4)\n```\n\n::: {.cell-output-display}\n![We can see that our linear model does not do a good job at capturing the curvature in our data.](index_files/figure-html/fig-fit-non-linear-1.png){#fig-fit-non-linear width=768}\n:::\n:::\n\n\n\nAs such, various extensions of linear models have been developed which both relax the assumption of linearity while also maintaining as much interpretability as possible.\n\n## Polynomial regression\n\nPolynomial regression is a natural extension of the standard linear model, where the relationship between the predictor $X$ and the response $Y$ is modeled as an $n$-th degree polynomial. This allows the model to capture curvature in the data while still using a linear framework for estimation. For instance, a second degree polynomial could be written as follows:\n\n$$y_i \\sim N(\\mu_i, \\; \\sigma^2) \\qquad  \\mu_i= \\alpha + \\beta_1x_{1i} + \\beta_2x_{1i}^2 + \\epsilon$$\n\nHere, $x_1, x_1^2$ are the polynomial terms, and $\\beta_1, \\beta_2$ are their corresponding coefficients. \n\nWe can continue this process for any finite polynomial degree such that:\n\n$$\\mu_i = \\alpha + \\sum^K_{k = 1} \\beta_kx_i^k + \\epsilon$$\n\nDespite the inclusion of nonlinear terms, polynomial regression remains linear in terms of estimation because the regression function is linear in the unknown parameters $(\\alpha, \\beta_1, \\dots, \\beta_k)$. This key property allows us to leverage the techniques of multiple linear regression for estimation and inference. Specifically, polynomial regression can be implemented by treating $x, \\dots,x^k$ as distinct independent variables in a multiple regression framework. As a result, the computational and inferential challenges of polynomial regression can be fully addressed using well-established methods, such as ordinary least squares.\n\n#### Refitting the Model with a Second-Degree Polynomial\n\nLet's refit a model to our data using a **second-degree polynomial**. To recap, such polynomial models take the form:\n\n$$Y \\sim N(\\mu_i \\: \\sigma^2)  \\qquad \\mu_i= \\alpha + \\beta_1x_{1i} + \\beta_2x_{1i}^2 + \\epsilon$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_2 + geom_smooth(\n  formula = y ~ poly(x, 2), method = \"lm\", \n  colour = \"forestgreen\", linewidth = 1.25, se = FALSE)\n```\n\n::: {.cell-output-display}\n![A second-degree polynomial fit (green) compared to the linear model (red). The polynomial model captures the curvature in the data, providing a much better fit.](index_files/figure-html/fig-refit-non-linear-1.png){#fig-refit-non-linear width=768}\n:::\n:::\n\n\n\n#### Exploring Higher-Degree Polynomials\n\nWhile a second-degree polynomial often provides a good balance between flexibility and simplicity, higher-degree polynomials can be used to capture more complex patterns in the data. However, care must be taken to avoid over fitting, where the model becomes too complex and starts capturing noise rather than the underlying trend.\n\nBelow are examples of fitting **third-degree** and **ninth-degree** polynomials to the same data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\np_3 <- p_3 + geom_smooth(\n  formula = y ~ poly(x, 3), method = \"lm\", \n  colour = \"forestgreen\", linewidth = 1.25, se = FALSE)\n\np_4 <- p_4 + geom_smooth(\n  formula = y ~ poly(x, 9), method = \"lm\",\n  colour = \"forestgreen\", linewidth = 1.25, se = FALSE)\n\np_3 + p_4\n```\n\n::: {.cell-output-display}\n![A third-degree (left) and ninth-degree (right) polynomial fit (green) compared to the linear model (red). As you can see each model fits the data well however may run the risk of overfitting on new data.](index_files/figure-html/fig-overfitting-1.png){#fig-overfitting width=768}\n:::\n:::\n\n\n\n\n## Splines and basis functions\n\nPolynomial regression can be seen as a special case of basis function regression, a more general framework for modeling nonlinear relationships. In basis function regression, we represent the nonlinear function of the predictor variable $X$ as a linear combination of simpler functions, known as basis functions. These basis functions allow us to flexibly model complex relationships while retaining the computational advantages of linear regression.\n\nFor example, in the case of a nonlinear regression with one predictor variable $X$ and a normally distributed outcome $Y$, the basis function regression model can be written as:\n\n$$\\mu_i = f(x_i) = \\alpha + \\sum^K_{k=1} \\beta_k \\phi_k(x_i)$$\n\nwhere $\\phi_k(x_i)$ are the basis functions, which are simple deterministic functions of $X$.\n\nIn polynomial regression, the basis functions are defined as:\n\n$$\\phi_k(x_i) = x_i^k $$\nFor example:\n\n- $\\phi_1(x_i) = x_i$ is a linear term\n- $\\phi_2(x_i) = x_i^2$ is a quadratic term\n- $\\phi_3(x_i) = x_i^3$ is a cubic term\n\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}