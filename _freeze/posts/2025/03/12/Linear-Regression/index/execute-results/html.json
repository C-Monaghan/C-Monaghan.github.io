{
  "hash": "3204662fa6a5641bb510d6d8b5e9bf0b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Doing linear regression by hand\"\ndescription: \"A step-by-step guide to calculating regression coefficients by hand\"\ndate: \"2025-03-12\"\nformat:\n  html:\n    html-math-method: katex\n    shift-heading-level-by: 1\n    lightbox: true\ncategories:\n  - statistics\n  - regression\n  - r\nimage: \"../../../thumbnails/regression_model_thumb.png\"\ndoi: 10.59350/nj895-ta783\ncitation: true\nexecute:\n  warning: false\n---\n\nFor the past few years I've been a statistics tutor in the psychology department at Maynooth University. We cover everything from the basics of descriptive statistics and graphs to the nitty-gritty of t-tests, correlations, and ANOVAs. However, there's one topic where I always slow down and dive deep: regression analysis. Why? Because linear regression is basically the Swiss Army knife of statistics—it’s everywhere, it’s versatile, and it’s way cooler than it sounds.\n\nSeriously, linear regression is like that one friend who shows up at every party. Lectures? Check. Assignments? Obviously. Research papers? You bet. Even casual chats about data somehow always circle back to it. It’s the ultimate tool for understanding how variables play together. Whether you’re predicting exam scores based on study hours (or lack thereof) or calculating how much coffee it takes to survive finals week (spoiler: *a lot*), regression has your back.\n\nOne thing I always like doing is having students calculate regression coefficients by hand. It's a great way to understand the mechanics of the model and appreciate the magic of least squares. But, full disclosure: I *always* forget the formulas mid-session and end up frantically Googling them. So, I decided to write them down here—partly for fun, partly as a future reference for myself ([hey future me!!](https://media.tenor.com/0CpFOKGVaeMAAAAi/hand-waving-hand.gif])).\n\n# The linear regression model\n\nAt its core, linear regression is all about finding relationships between variables. The model is a simple equation that describes how a dependent variable $(Y)$ relates to one or more independent variables $(X)$. In its simplest form, it looks like this:\n\n$$ Y = \\beta_0 + \\beta_1X + \\epsilon $$\n\nwhere:\n\n- $Y$ the dependent variable (the thing you’re trying to predict).\n- $X$ the independent variable (the thing you’re using to make predictions).\n- $\\beta_0$ is the intercept (where the line crosses the Y-axis).\n- $\\beta_1$ is the slope (how much $Y$ changes for every unit change in $X$).\n- $\\epsilon$ the error term (because let’s face it, life is messy).\n\n## Generating some data\n\nFirst let's load some packages and set our theme\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loading packages\nlibrary(ggplot2)\n\n# Setting default theme\ntheme_set(\n  theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = rel(1.4)),\n      axis.title = element_text(size = rel(1))\n      )\n)\n```\n:::\n\n\nTo make this more concrete, let’s create a small dataset in R. We’ll simulate some data for study hours and exam scores, and then use linear regression to model the relationship between them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\n\n# Hours of study per week (our x variable)\nstudy_hours <- rnorm(10, mean = 7, sd = 2)\n\n# Exam scores (our y variable)\nexam_scores <- 25 + 5 * study_hours + rnorm(10, mean = 0, sd = 2)\n\ntest_data <- data.frame(\n  study_hours = ceiling(study_hours),\n  exam_scores = ceiling(exam_scores))\n\nhead(test_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  study_hours exam_scores\n1           6          57\n2           7          59\n3          11          77\n4           8          61\n5           8          61\n6          11          81\n```\n\n\n:::\n:::\n\n\nIf we plot this data we can see the relationship between study hours and exam scores:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexam_plot <- test_data |>\nggplot(aes(x = study_hours, y = exam_scores)) +\n  geom_point(size = 4) +\n  labs(\n    title = \"Exam scores vs. study hours\",\n    x = \"Study hours\",\n    y = \"Exam scores\"\n  )\n\nexam_plot\n```\n\n::: {.cell-output-display}\n![Scatter plot of exam scores vs. study hours. Each point represents a student's exam score based on the number of study hours per week.](index_files/figure-html/fig-exam-scores-1.png){#fig-exam-scores width=1152}\n:::\n:::\n\n\nFrom the plot, you can see a clear trend: **the more hours a student spends studying, the higher their exam score tends to be**. This is the relationship we want to model using linear regression.\n\n## Fitting a model in R\n\nIn R, fitting a linear regression model is as simple as using the `lm()` function. We provide the formula for the model and the dataset, and R does the rest. Let's fit a linear model to our data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(exam_scores ~ study_hours, data = test_data)\n\nbroom::tidy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)    21.3      3.93       5.42 0.000627  \n2 study_hours     5.23     0.496     10.5  0.00000569\n```\n\n\n:::\n:::\n\n\nThe `lm()` function fits a linear model where `exam_scores` is the dependent variable $(Y)$ and `study_hours` is the independent variable $(X)$. The output gives us the estimated coefficients for the intercept $(\\beta_0)$ and slope $(\\beta_1)$.\n\n- The intercept $(\\beta_0)$ is **21.33**, meaning a student who studies 0 hours is predicted to score approximately 21% on the exam.\n- The slope $(\\beta_1)$ is **5.23**, indicating that for every additional hour of study, a student's predicted exam score increases by 5.23%.\n\nVisually this looks like\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexam_plot <- exam_plot +\n  geom_smooth(\n    method = \"lm\",\n    colour = ggokabeito::palette_okabe_ito(order = 5),\n    se = FALSE) +\n  # Adding equation of the line\n    annotate(\n      \"text\", x = 6, y = 80,\n    label = paste0(\n      \"Y = \", round(coef(fit)[1], 2), \" + \", round(coef(fit)[2], 2), \"X\"),\n    color = ggokabeito::palette_okabe_ito(order = 5),\n    size = rel(5), fontface = \"bold\"\n  )\n\nexam_plot\n```\n\n::: {.cell-output-display}\n![Scatter plot of exam scores vs. study hours with a linear regression line. The equation of the line is displayed on the plot.](index_files/figure-html/fig-exam-scores-regression-1.png){#fig-exam-scores-regression width=1152}\n:::\n:::\n\n\n**But how are these values calculated**\n\n# Calculating Regression Coefficients by Hand\n\nWhile R’s `lm()` function makes fitting a linear regression model incredibly easy, it’s helpful to understand how the coefficients $(\\beta_0 \\: \\text{and} \\: \\beta_1)$ are derived.\n\nThe goal of linear regression is to find the best-fitting line through the data, which is done by minimizing the sum of squared errors. The sum of squared errors is calculated as:\n\n$$ SSE = \\sum_{i=1}^{n} (Y_i - \\hat{Y_i})^2 $$\n\nwhere:\n\n- $Y_i$ is the actual value of the dependent variable.\n- $\\hat{Y_i}$ is the predicted value of the dependent variable.\n- $n$ is the number of observations.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Add predicted values to the dataset\ntest_data <- test_data |>\n  dplyr::mutate(predicted = coef(fit)[1] + coef(fit)[2] * study_hours)\n\n# Visualizing SSE\nexam_plot +\n  geom_point(\n    data = test_data,\n    aes(x = study_hours, y = predicted),\n    size = 2.5,\n    colour = ggokabeito::palette_okabe_ito(order = 1)) +\n  geom_segment(\n  data = test_data,\n  aes(x = study_hours, xend = study_hours,\n      y = exam_scores,  yend = predicted),\n  colour = ggokabeito::palette_okabe_ito(order = 2),\n  linewidth = 1.25, linetype = \"dashed\", alpha = 0.7\n)\n```\n\n::: {.cell-output-display}\n![Visual representation of the sum of squared errors (SSE) in linear regression. The goal is to minimize the vertical distance between the data points and the regression line.](index_files/figure-html/fig-SSE-1.png){#fig-SSE width=1152}\n:::\n:::\n\n\nTo minimize the SSE, we need to find the values of $\\beta_0$ (intercept) and $\\beta_1$ (slope) that make the line fit the data as closely as possible. The formulas for $\\beta_0$ and $\\beta_1$ are:\n\n\\begin{align*}\n\\beta_1 &= \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - X)^2} \\\\[18pt]\n\n\\beta_0 &= \\bar{Y} - \\beta_1\\bar{X}\n\\end{align*}\n\nwhere:\n\n- $X_i$ and $Y_i$ are the individual data points.\n- $\\bar{X}$ and $\\bar{Y}$ are the means of the independent and dependent variables, respectively.\n\n## Step 1: Calculate the means\n\nThe first step is to calculate the means of $X$ and $Y$. These are the averages of the independent and dependent variables, respectively.\n\n\\begin{align*}\n\\bar{X} = \\frac{\\sum X_i}{n} = \\frac{77}{10} = 7.7 \\\\[12pt]\n\n\\bar{Y} = \\frac{\\sum Y_i}{n} = \\frac{616}{10} = 61.6\n\\end{align*}\n\n## Step 2: Calculate the deviations from the mean\n\nNext, we calculate the deviations from the mean for each data point and their products. These are the building blocks for the slope ($\\beta_1)$\n\n| Student | $X$ |  $Y$ | $(x_i - \\bar{x})(y_i - \\bar{y})$ | $(x_i - \\bar{x})^2$ |\n|:-------:|:---:|:----:|:--------------------------------:|:-------------------:|\n| 1       | 6   | 57   | 7.82                             | 2.89                |\n| 2       | 7   | 59   | 1.82                             | 0.49                |\n| 3       | 11  | 77   | 50.82                            | 10.89               |\n| 4       | 8   | 61   | -0.18                            | 0.09                |\n| 5       | 8   | 61   | -0.18                            | 0.09                |\n| 6       | 11  | 81   | 64.02                            | 10.89               |\n| 7       | 8   | 66   | 1.32                             | 0.09                |\n| 8       | 5   | 44   | 47.52                            | 7.29                |\n| 9       | 6   | 55   | 11.22                            | 2.89                |\n| 10      | 7   | 55   | 4.62                             | 0.49                |\n| $\\sum$  | 77  | 616  | 188.8                            | 36.1                |\n\n\n## Step 3: Calculate the slope $(\\beta_1)$\n\nUsing the formula for $\\beta_1$ we now get:\n\n\\begin{align*}\n\\beta_1 &= \\frac{\\sum(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum(X_i - X)^2} \\\\[12pt]\n\n&= \\frac{188.8}{36.1} \\\\[12pt]\n\n&\\approx 5.23\n\\end{align*}\n\n## Step 4: Calculating the intercept $(\\beta_0)$\n\nUsing the formula for $\\beta_0$ we now get:\n\n\\begin{align*}\n\\beta_0 &= \\bar{Y} - \\beta_1\\bar{X} \\\\[12pt]\n\n&= 61.6 - 5.23(7.7) \\\\[12pt]\n\n&\\approx 21.32\n\\end{align*}\n\n## Step 5: Writing our regression equation\n\nNow that we have values for $\\beta_0 \\: \\text{and} \\: \\beta_1$ we can write the regression equation:\n\n$$\\hat{Y} = 21.3 + 5.23X$$\n\nwhich if you remember from before is the same as our `lm()` model in R.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbroom::tidy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  <chr>          <dbl>     <dbl>     <dbl>      <dbl>\n1 (Intercept)    21.3      3.93       5.42 0.000627  \n2 study_hours     5.23     0.496     10.5  0.00000569\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}