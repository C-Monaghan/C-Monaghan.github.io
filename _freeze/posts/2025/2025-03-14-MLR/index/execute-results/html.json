{
  "hash": "f236f141a91a8ddf58ca419e8ef5fab7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Doing multiple linear regression by hand\"\ndescription: \"A step-by-step guide to calculating regression coefficients by hand\"\ndate: \"2025-03-14\"\nformat:\n  html:\n    html-math-method: katex\n    shift-heading-level-by: 1\n    lightbox: true\ncategories:\n  - statistics\n  - regression\n  - r\nimage: \"../thumbnails/regression_mlr_thumb.png\"\ncitation:\n  url: \"https://c-monaghan.github.io/posts/2025/2025-03-14-MLR/\"\nexecute: \n  warning: false\neditor: source\n---\n\n\n\nIn my last post I showed how to do [simple linear regression by hand](https://c-monaghan.github.io/posts/2025/2025-03-12-Linear-Regression/). However, naturally, in most cases with statistics, you never have just one predictor variable, but rather multiple. As such, in this post, I will show how to do multiple linear regression by hand. Multiple linear regression is an extension of simple linear regression that allows for more than one predictor variable. It’s a powerful tool for understanding the relationship between several independent variables and a single dependent variable.\n\nWhile the calculations can get a bit more involved, the core idea remains the same: we’re trying to find the best-fitting linear relationship between our predictors and the outcome.\n\n# What is multiple linear regression?\n\nIf simple linear regression is like baking a cake with just flour, multiple linear regression is like adding sugar, eggs, and butter to the mix. It’s a richer and more complex way of understanding relationships in your data.\n\nIn the real world, things are rarely influenced by just one factor. For example, your happiness isn’t just determined by how much coffee you drink (though that’s a big part of it). It’s also influenced by how much sleep you got, how many emails are in your inbox, and whether your cat decided to knock over your favorite mug. Multiple linear regression helps us model these kinds of multi-faceted relationships.\n\n## The equation\n\nAt its core, multiple linear regression is about finding the best-fitting linear relationship between **multiple predictor variables** (independent variables) and a **response variable** (dependent variable). The equation is similar to that of simple linear regression, but with more predictor variables:\n\n$$\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_jX_j + \\epsilon\n$$\n\nWhere:\n\n- $Y$ is your dependent variable—the thing you’re trying to predict or explain. (Think: your happiness score.)\n- $\\beta_0$ is the **intercept**. It’s the value of $Y$ when all the predictor variables are zero. (If you drank no coffee, got zero sleep, and had no emails, how happy would you be? Probably not very.)\n- $\\beta_1, \\beta_2, \\dots, \\beta_j$ are the **coefficients** for each predictor variable $(X_1, X_2, \\dots, X_j)$. They tell you how much each predictor contributes to YY. (For example, does coffee make you happier than sleep? These coefficients will tell you!)\n- $\\epsilon$ is the error term. It accounts for the randomness in life that the model can’t explain. (Maybe your cat did something unpredictable, like actually not knocking over your mug.)\n\nThe goal of multiple linear regression is to find the values of $\\beta_0, \\beta_1, \\dots, \\beta_j$ that minimize the sum of squared errors—basically, the difference between what the model predicts $(\\hat{Y})$ and what actually happens $(Y)$. In other words, **we’re trying to make our predictions as accurate as possible**.\n\n## Generating some data\n\nFirst, let's again load some packages and set up our theme.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Loading packages\nlibrary(ggplot2)    # For plotting\nlibrary(ggsci)      # Going with a simpsons theme \nlibrary(gghalves)   # Using half plots\nlibrary(dplyr)      # Data manipulation\nlibrary(tidyr)      # Data manipulation\n\n# Setting default theme\ntheme_set(\n  theme_minimal() +\n    theme(\n      panel.grid.minor = element_blank(),\n      plot.title = element_text(hjust = 0.5, face = \"bold\", size = rel(1.4)),\n      axis.title = element_text(face = \"bold\", size = rel(1)),\n      strip.background = element_rect(fill = \"grey80\", color = NA)\n      )\n)\n```\n:::\n\n\n\n\nTo illustrate multiple linear regression, let’s generate some data. For this example, let's stick with the happiness theme we have going. We’ll create a dataset with three predictor variables $(X_1, X_2, X_3)$ representing daily coffee intake, sleep, and emails, and one response variable $(Y)$ representing daily happiness.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(321)\n\n# Generating our predictor variables\ncoffee <- rnorm(10, mean = 2, sd = 1)   # Coffee intake (cups per day)\nsleep  <- rnorm(10, mean = 7, sd = 1)   # Sleep (hours per night)\nemails <- rnorm(10, mean = 10, sd = 3)  # Emails (received per day)\n\n# Generating our response variable\nhappiness <- 5 + 0.2 * coffee + 0.7 * sleep - 0.2 * emails + rnorm(10, mean = 0, sd = 1)\n\n# Creating data set\nhappiness_data <- data.frame(\n  happiness = ceiling(happiness),\n  coffee    = floor(coffee),\n  sleep     = ceiling(sleep),\n  emails    = ceiling(emails)\n)\n\nhead(happiness_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  happiness coffee sleep emails\n1         7      3     8     13\n2        10      1     9     10\n3         9      1     8     13\n4        12      1    10      7\n5         8      1     6      8\n6         8      2     7     11\n```\n\n\n:::\n\n```{.r .cell-code}\nView(happiness_data)\n```\n:::\n\n\n\nBefore we dive into regression, let’s take a moment to visualize our data. We’ll use a combination of boxplots and jittered points to show the distribution of each variable\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhappiness_long <- happiness_data |>\n  pivot_longer(\n    cols = everything(),\n    names_to = \"variable\",\n    values_to = \"value\") |>\n  mutate(variable = factor(\n    variable, levels = c(\"coffee\", \"sleep\", \"emails\", \"happiness\")))\n\nhappiness_plot <- happiness_long |>\n  ggplot(aes(x = variable, y = value, fill = variable, colour = variable)) +\n  gghalves::geom_half_point(\n    transformation = ggbeeswarm::position_quasirandom(width = 0.1), \n    side = \"r\") +\n  gghalves::geom_half_boxplot(side = \"l\", colour = \"black\") +\n  scale_fill_simpsons() +\n  scale_colour_simpsons() +\n  scale_y_continuous(breaks = seq(2, 12, by = 2)) +\n  labs(x = \"Variables\", y = \"Value\") +\n  guides(fill = \"none\", colour = \"none\")\n\nhappiness_plot\n```\n\n::: {.cell-output-display}\n![Visualising the distribution of our variables](index_files/figure-html/fig-visualisation-1.png){#fig-visualisation width=1152}\n:::\n:::\n\n\n\n**What Do We See?**\n\nFrom the plot, we can start to get a sense of how our variables are distributed:\n\n- Coffee intake hovers around 2-3 cups per day.\n- Sleep is mostly in the 7-8 hour range.\n- Emails are all over the place, because inboxes are chaos.\n- Happiness scores are a bit more spread out, reflecting the combined influence of coffee, sleep, and emails.\n\nThis visualization sets the stage for our regression analysis. Next up, we’ll dive into the math and find out exactly how much each of these factors contributes to happiness.\n\n# Calculating coefficients (using equations)\n\nWe'll calculate the coefficients for the regression equation of the form:\n\n$$\n\\text{happiness} = \\beta_0 + \\beta_1(\\text{coffee}) + \\beta_2(\\text{sleep}) + \\beta_3(\\text{emails})\n$$\n\nIn order to obtain these coefficients we use least squares estimation. This involves finding the values of $\\beta_0, \\beta_1, \\beta_2, \\beta_3$ that minimize the sum of squared errors between the observed happiness scores and the predicted happiness scores. \n\n**Our formulas are**:\n\n\\begin{align*}\n\\beta_0 &= \\bar{y} - \\beta_1 \\bar{x_1} - \\beta_2 \\bar{x_2} - \\beta_3 \\bar{x_3} \\\\[12pt]\n\n\\beta_1 &= \\frac{S_{x_1 y} S_{x_2 x_2} S_{x_3 x_3} - S_{x_2 y} S_{x_1 x_2} S_{x_3 x_3} - S_{x_3 y} S_{x_1 x_3} S_{x_2 x_2} + S_{x_2 y} S_{x_1 x_3} S_{x_2 x_3} + S_{x_3 y} S_{x_1 x_2} S_{x_2 x_3} - S_{x_1 y} S_{x_2 x_3}^2}{S_{x_1 x_1} S_{x_2 x_2} S_{x_3 x_3} - S_{x_1 x_2}^2 S_{x_3 x_3} - S_{x_1 x_3}^2 S_{x_2 x_2} + 2 S_{x_1 x_2} S_{x_1 x_3} S_{x_2 x_3} - S_{x_2 x_3}^2 S_{x_1 x_1}} \\\\[12pt]\n\n\\beta_2 &= \\frac{S_{x_2 y} S_{x_1 x_1} S_{x_3 x_3} - S_{x_1 y} S_{x_1 x_2} S_{x_3 x_3} - S_{x_3 y} S_{x_1 x_1} S_{x_2 x_3} + S_{x_1 y} S_{x_1 x_3} S_{x_2 x_3} + S_{x_3 y} S_{x_1 x_2} S_{x_1 x_3} - S_{x_2 y} S_{x_1 x_3}^2}{S_{x_1 x_1} S_{x_2 x_2} S_{x_3 x_3} - S_{x_1 x_2}^2 S_{x_3 x_3} - S_{x_1 x_3}^2 S_{x_2 x_2} + 2 S_{x_1 x_2} S_{x_1 x_3} S_{x_2 x_3} - S_{x_2 x_3}^2 S_{x_1 x_1}} \\\\[12pt]\n\n\\beta_3 &= \\frac{S_{x_3 y} S_{x_1 x_1} S_{x_2 x_2} - S_{x_1 y} S_{x_1 x_3} S_{x_2 x_2} - S_{x_2 y} S_{x_1 x_1} S_{x_2 x_3} + S_{x_1 y} S_{x_1 x_2} S_{x_2 x_3} + S_{x_2 y} S_{x_1 x_3} S_{x_1 x_2} - S_{x_3 y} S_{x_1 x_2}^2}{S_{x_1 x_1} S_{x_2 x_2} S_{x_3 x_3} - S_{x_1 x_2}^2 S_{x_3 x_3} - S_{x_1 x_3}^2 S_{x_2 x_2} + 2 S_{x_1 x_2} S_{x_1 x_3} S_{x_2 x_3} - S_{x_2 x_3}^2 S_{x_1 x_1}}\n\n\n\\end{align*}\n\nwhere:\n\n\\begin{align*}\nS_{x_1 y} &= \\sum (x_1 - \\bar{x_1})(y - \\bar{y}) \\quad \\text{(cross-deviations between coffee and happiness)} \\\\\nS_{x_2 y} &= \\sum (x_2 - \\bar{x_2})(y - \\bar{y}) \\quad \\text{(cross-deviations between sleep and happiness)} \\\\\nS_{x_3 y} &= \\sum (x_3 - \\bar{x_3})(y - \\bar{y}) \\quad \\text{(cross-deviations between emails and happiness)} \\\\\nS_{x_1 x_1} &= \\sum (x_1 - \\bar{x_1})^2 \\quad \\text{(squared deviations for coffee)} \\\\\nS_{x_2 x_2} &= \\sum (x_2 - \\bar{x_2})^2 \\quad \\text{(squared deviations for sleep)} \\\\\nS_{x_3 x_3} &= \\sum (x_3 - \\bar{x_3})^2 \\quad \\text{(squared deviations for emails)} \\\\\nS_{x_1 x_2} &= \\sum (x_1 - \\bar{x_1})(x_2 - \\bar{x_2}) \\quad \\text{(cross-deviations between coffee and sleep)} \\\\\nS_{x_1 x_3} &= \\sum (x_1 - \\bar{x_1})(x_3 - \\bar{x_3}) \\quad \\text{(cross-deviations between coffee and emails)} \\\\\nS_{x_2 x_3} &= \\sum (x_2 - \\bar{x_2})(x_3 - \\bar{x_3}) \\quad \\text{(cross-deviations between sleep and emails)}\n\\end{align*}\n\n\n## Step 1: Computing means\n\nFirst, calculate the means of happiness, coffee, sleep, and emails.\n\n\\begin{align*}\n\\bar{y}   &= \\frac{\\sum y_i}{n}    = \\frac{7 + 10 + 9 + \\dots + 9}{10}   = \\frac{92}{10} =  9.2 \\\\[12pt]\n\\bar{x_1} &= \\frac{\\sum x_{1i}}{n} = \\frac{3 + 1 + 1 + \\dots + 1}{10}    = \\frac{16}{10} = 1.6  \\\\[12pt]\n\\bar{x_2} &= \\frac{\\sum x_{2i}}{n} = \\frac{8 + 9 + 8 + \\dots + 8}{10}    = \\frac{80}{10} = 8    \\\\[12pt]\n\\bar{x_3} &= \\frac{\\sum x_{3i}}{n} = \\frac{13 + 10 + 13 + \\dots + 9}{10} = \\frac{94}{10} = 9.4 \n\\end{align*}\n\n## Step 2: Computing deviations\n\nNext, we calculate the deviations for each variable from their mean.\n\n| Observation | $y - \\bar{y}$      | $x_1 - \\bar{x_1}$ | $x_2 - \\bar{x_2}$   | $x_3 - \\bar{x_3}$   |\n|-------------|--------------------|-------------------|---------------------|---------------------|\n| 1           | 7 - 9.2 = -2.2     | 3 - 1.6 = 1.4     | 8 - 8 = 0           | 13 - 9.4 = 3.6      |\n| 2           | 10 - 9.2 = 0.8     | 1 - 1.6 = -0.6    | 9 - 8 = 1           | 10 - 9.4 = 0.6      |\n| 3           | 9 - 9.2 = -0.2     | 1 - 1.6 = -0.6    | 8 - 8 = 0           | 13 - 9.4 = 3.6      |\n| 4           | 12 - 9.2 = 2.8     | 1 - 1.6 = -0.6    | 10 - 8 = 2          | 7 - 9.4 = -2.4      |\n| 5           | 8 - 9.2 = -1.2     | 1 - 1.6 = -0.6    | 6 - 8 = -2          | 8 - 9.4 = -1.4      |\n| 6           | 8 - 9.2 = -1.2     | 2 - 1.6 = 0.4     | 7 - 8 = -1          | 11 - 9.4 = 1.6      |\n| 7           | 8 - 9.2 = -1.2     | 2 - 1.6 = 0.4     | 8 - 8 = 0           | 4 - 9.4 = -5.4      |\n| 8           | 11 - 9.2 = 1.8     | 2 - 1.6 = 0.4     | 8 - 8 = 0           | 12 - 9.4 = 2.6      |\n| 9           | 10 - 9.2 = 0.8     | 2 - 1.6 = 0.4     | 8 - 8 = 0           | 7 - 9.4 = -2.4      |\n| 10          | 9 - 9.2 = -0.2     | 1 - 1.6 = -0.6    | 8 - 8 = 0           | 9 - 9.4 = -0.4      |\n\n\n## Step 3: Computing cross deviations\n\nNow that we know our means and deviations we are able to solve for our set of cross-deviations.\n\n\\begin{align*}\nS_{x_1 y} &= (1.4 \\times -2.2) + (-0.6 \\times 0.8) + (-0.6 \\times -0.2) + \\dots (-0.6 \\times -0.2) = -4.2 \\\\[12pt]\n\nS_{x_2 y} &= (0 \\times -2.2) + (1 \\times 0.8) + (0 \\times -0.2) + \\dots + (0 \\times -0.2) = 10 \\\\[12pt]\n\nS_{x_3 y} &= (3.6 \\times -2.2) + (0.6 \\times 0.8) + (3.6 \\times -0.2) \\dots + (-0.4 \\times -0.2) = -5.8 \\\\[24pt]\n\nS_{x_1x_2} &= (1.4 \\times 0) + (-0.6 \\times 1) + (-0.6 \\times 0) + \\dots (-0.6 \\times 0) = -1 \\\\[12pt]\n\nS_{x_1x_3} &= (1.4 \\times 3.6) + (-0.6 \\times 0.6) + (-0.6 \\times 3.6) + \\dots + (-0.6 \\times -0.4) = 3.6 \\\\[12pt]\n\nS_{x_2x_3} &= (0 \\times 3.6) + (1 \\times 0.6) + (0 \\times 3.6) + \\dots (0 \\times -0.4) = -3\n\\end{align*}\n\n## Step 4: Computing squared deviations\n\nFinally, we can calculate the squared deviations for each variable.\n\n\\begin{align*}\nS_{x_1 x_1} &= (1.4^2) + (-0.6^2) + (-0.6^2) + \\dots + (-0.6^2) = 4.4 \\\\[12pt]\n\nS_{x_2 x_2} &= (0^2) + (1^2) + (0^2) + \\dots + (0^2) = 10 \\\\[12pt]\n\nS_{x_3 x_3} &= (3.6^2) + (0.6^2) + (3.6^2) + \\dots + (-0.4^2) = 78.4 \\\\[12pt]\n\\end{align*}\n\n## Step 5: Computing our coefficients\n\nNow that we have all the necessary values, we can calculate our coefficients.\n\n\\begin{align*}\n\\beta_1 &= \\frac{-4.2 \\times 10 \\times 78.4 - 10 \\times -1 \\times -3 - 5.8 \\times 4 \\times 10 + 10 \\times 3.6 \\times -3 - 5.8 \\times -1 \\times -3 + 4.2 \\times (-3)^2}{4.4 \\times 10 \\times 78.4 - 10 \\times (-1)^2 \\times 78.4 - 10 \\times 3.6^2 + 2 \\times -1 \\times 3.6 \\times -3 - 3^2 \\times 10} \\approx -0.74 \\\\[12pt]\n\n\\beta_2 &= \\frac{10 \\times 4.4 \\times 78.4 - -4.2 \\times -1 \\times 78.4 - 5.8 \\times 4.4 \\times -3 + -4.2 \\times 3.6 \\times -3 + 5.8 \\times -1 \\times -1 - 10 \\times (-3)^2}{4.4 \\times 10 \\times 78.4 - 10 \\times (-1)^2 \\times 78.4 - 10 \\times 3.6^2 + 2 \\times -1 \\times 3.6 \\times -3 - 3^2 \\times 10} \\approx 0.92 \\\\[12pt]\n\n\\beta_3 &= \\frac{-5.8 \\times 4.4 \\times 10 - -4.2 \\times 3.6 \\times 10 - 10 \\times 4.4 \\times -3 + -4.2 \\times -1 \\times -3 + 10 \\times 3.6 \\times -1 - 10 \\times (-3)^2}{4.4 \\times 10 \\times 78.4 - 10 \\times (-1)^2 \\times 78.4 - 10 \\times 3.6^2 + 2 \\times -1 \\times 3.6 \\times -3 - 3^2 \\times 10} \\approx -0.005 \\\\[12pt]\n\n\\beta_0 &= 9.2 - (-0.74 \\times 1.6) - 0.92 \\times 8 - (-0.005 \\times 9.4) = 9.2 + 1.924 - 7.36 + 0.047 \\approx 3.81\n\\end{align*}\n\n## Step 6: Writing our regression equation\n\nFinally, we can write our regression equation:\n\n$$\\hat{Y} = 3.07 - 0.74X_1 + 0.92X_2 - 0.005X_3$$\nPhew, that was a lot of work!!\n\n**But what does this now tells us??**\n\n- For every additional cup of coffee you drink, your happiness score decreases by 0.74.\n- For every additional hour of sleep you get, your happiness score increases by 0.92.\n- For every additional email you receive, your happiness score decreases by 0.005.\n\nLet's verify this by running the `lm()` function in R\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit <- lm(happiness ~ coffee + sleep + emails, data = happiness_data)\n\nbroom::tidy(fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  3.03        3.75     0.808   0.450 \n2 coffee      -0.741       0.608   -1.22    0.269 \n3 sleep        0.925       0.398    2.32    0.0591\n4 emails      -0.00459     0.143   -0.0321  0.975 \n```\n\n\n:::\n:::\n\n\n\nThe coefficients are very close to what we calculated by hand. \n\n# Calculating coefficients (using matrices)\n\nWhile the manual method of calculating coefficients for multiple linear regression is a great way to understand the underlying mechanics, it can become tedious, especially when dealing with multiple predictor variables. Imagine having five or six predictors—the calculations would quickly become overwhelming! Fortunately, matrices provide a more efficient and streamlined approach.\n\n## Step 1: Setting up our matrices\n\nTo begin, let’s define the key components of our regression model:\n\n- $y$ as the response variable\n- $X$ as the matrix of predictor variables\n- $\\beta$ as the vector of coefficients we want to estimate\n- $\\epsilon$ as the error term\n\nThe regression model can be expressed in matrix form as:\n\n$$y = X \\beta + \\epsilon$$\n\nOur goal is to solve for $\\beta$, the vector of coefficients. The least squares solution for $\\beta$ is given by:\n\n$$\\beta = (X^TX)^{-1} X^Ty$$\nLet's define the $X$ matrix (including our intercept) and $y$ matrix\n\n$$\nX = \\begin{bmatrix}\n1 & 3 & 8 & 13 \\\\\n1 & 1 & 9 & 10 \\\\\n1 & 1 & 8 & 13 \\\\\n1 & 1 & 10 & 7 \\\\\n1 & 1 & 6 & 8 \\\\\n1 & 2 & 7 & 11 \\\\\n1 & 2 & 8 & 4 \\\\\n1 & 2 & 8 & 12 \\\\\n1 & 2 & 8 & 7 \\\\\n1 & 1 & 8 & 9 \\\\\n\\end{bmatrix} \\qquad\ny = \\begin{bmatrix}\n7 \\\\\n10 \\\\\n9 \\\\\n12 \\\\\n8 \\\\\n8 \\\\\n8 \\\\\n11 \\\\\n10 \\\\\n9 \\\\\n\\end{bmatrix}\n$$\n\n## Step 2: Transposing $X$\n\nThe next step is to compute the transpose of $X$ denoted as $X^T$. Transposing a matrix involves flipping its rows and columns. For our $X$ matrix, this results in:\n\n$$\nX^T = \\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 \\\\\n3 & 1 & 1 & 1 & 1 & 2 & 2 & 2 & 2 & 1 \\\\\n8 & 9 & 8 & 10 & 6 & 7 & 8 & 8 & 8 & 8 \\\\\n13 & 10 & 13 & 7 & 8 & 11 & 4 & 12 & 7 & 9 \\\\\n\\end{bmatrix}\n$$\n\n## Step 3: Computing matrix products\n\nWith $X^T$ calculated, we now compute two key matrix products:\n\n- $X^TX$: This product of the transposed predictor matrix and original predictor matrix.\n- $X^Ty$: This product of the transposed predictor matrix and the response variable matrix.\n\nMatrix multiplication involves multiplying corresponding elements of each row and column. For example, the first element of the resulting matrix is the sum of the products of the first row of the first matrix $X^TX$ and the first column of the second matrix $X$. To illustrate, the first two elements of $X^TX$ would be:\n\n$$\n(1 \\times 1) + (1 \\times 1) + \\dots + (1 \\times 1) = 10 \\\\[8pt]\n(1 \\times 3) + (1 \\times 1) + \\dots + (1 \\times 1) = 16\n$$\n\nContinuing this process for all elements of $X^TX$ and $X^Ty$ results in:\n\n$$\nX^T X = \\begin{bmatrix}\n10 & 16 & 80 & 94 \\\\\n16 & 30 & 127 & 154 \\\\\n80 & 127 & 650 & 749 \\\\\n94 & 154 & 749 & 962 \\\\\n\\end{bmatrix} \\qquad\nX^T y = \\begin{bmatrix}\n92 \\\\\n143 \\\\\n746 \\\\\n859 \\\\\n\\end{bmatrix}\n$$\n\n\n## Step 4: Computing the inverse\n\nThe next step is to compute the inverse of $X^TX$, otherwise denoted as $(X^TX)^{-1}$. The inverse of a matrix is a special matrix that, when multiplied by the original matrix, yields the identity matrix. In other words:\n\n$$(X^TX)^{-1}(X^TX) = I$$\n\nwhere $I$ is the identity matrix. The inverse of a matrix can be computed using various methods, with [Gaussian elimination](https://en.wikipedia.org/wiki/Gaussian_elimination) being one of the most common. Gaussian elimination transforms the matrix into row echelon form through a series of row operations, ultimately reducing it to the identity matrix. \n\n- Start by augmenting $X^TX$ with an identity matrix $I$ such that $[X^TX \\: \\vert \\: I]$:\n\n$$\n[X^TX \\: \\vert \\: I] = \\begin{bmatrix}\n10 & 16 & 80 & 94 & \\vert & 1 & 0 & 0 & 0 \\\\\n16 & 30 & 127 & 154 & \\vert & 0 & 1 & 0 & 0 \\\\\n80 & 127 & 650 & 749 & \\vert & 0 & 0 & 1 & 0 \\\\\n94 & 154 & 749 & 962 & \\vert & 0 & 0 & 0 & 1 \\\\\n\\end{bmatrix}\n$$\n\n- Perform row operations to reduce the left side of the matrix to the identity matrix, resulting in the inverse of $X^TX$ on the right side.\n- The resulting matrix on the right side will be the inverse of $X^TX$.\n\nFor my own mental sanity I will not perform this operation by hand, but rather use the `solve()` function in R to calculate the inverse of $X^TX$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXT_X <- matrix(c(\n  10, 16, 80, 94,\n  16, 30, 127, 154,\n  80, 127, 650, 749,\n  94, 154, 749, 962\n), nrow = 4, ncol = 4, byrow = TRUE)\n\nsolve(XT_X) |> round(digits = 3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       [,1]   [,2]   [,3]   [,4]\n[1,]  9.162 -0.456 -0.885 -0.133\n[2,] -0.456  0.240  0.021 -0.010\n[3,] -0.885  0.021  0.103  0.003\n[4,] -0.133 -0.010  0.003  0.013\n```\n\n\n:::\n:::\n\n\n\nWe can then write $(X^TX)^{-1}$ as:\n\n$$\n(X^TX)^{-1} = \\begin{bmatrix}\n9.162 & -0.446 & -0.885 & -0.133 \\\\\n-0.456 & 0.240 & 0.021 & -0.010 \\\\\n-0.885 & 0.021 & 0.103 & 0.003 \\\\\n-0.133 & -0.010 & 0.003 & 0.013\n\\end{bmatrix}\n$$\n\n## Step 5: Computing $\\beta$\n\nWith $(X^TX)^{-1}$ calculated, we can now compute $\\beta$ using the formula:\n\n$$\\beta = (X^TX)^{-1} (X^Ty)$$\n\nMultiplying $(X^TX)^{-1}$ by $X^Ty$ gives us:\n\n$$\n\\beta = \\begin{bmatrix}\n3.239 \\\\\n-0.556 \\\\\n0.998 \\\\\n-0.261 \\\\\n\\end{bmatrix}\n$$\n\n## Step 6: Writing our regression equation\n\nFinally, we can write our regression equation:\n\n$$y = 3.239 - 0.556X_1 + 0.998X_2 - 0.261X_3$$\n\n**But wait, this isn't what we got by doing it by hand, nor does it match the `lm()` output.**\n\nThis is because the `solve()` function in R is able to calculate the inverse of a matrix without any rounding errors. This is not the case when doing it by hand. If I removed `|> round(digits = 3)` from the above we get a much more precise estimate of $(X^TX)^{-1}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsolve(XT_X)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           [,1]        [,2]         [,3]         [,4]\n[1,]  9.1623030 -0.45619804 -0.885469661 -0.132832858\n[2,] -0.4561980  0.24041444  0.020970344 -0.010237002\n[3,] -0.8854697  0.02097034  0.102990445  0.002978037\n[4,] -0.1328329 -0.01023700  0.002978037  0.013339124\n```\n\n\n:::\n:::\n\n\n \nand then ultimately an exact match to the `lm()` output\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nXT_y <- matrix(c(92, 143, 746, 859), nrow = 4, ncol = 1, byrow = TRUE)\n\nsolve(XT_X) %*% XT_y\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            [,1]\n[1,]  3.03176573\n[2,] -0.74066261\n[3,]  0.92455640\n[4,] -0.00459114\n```\n\n\n:::\n:::\n\n\n\nSo finally, we could say our regression equation is:\n\n$$y = 3.03 - 0.74X_1 + 0.92X_2 - 0.005X_3$$\n\nJust goes to show how much rounding makes a difference!\n\n\n\n\n::: {.cell}\n\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}